{
  
    
        "post0": {
            "title": "Graph Representations for Predictive Modeling in Traffic Scenes",
            "content": "Abstract . Some deep reinforcement learning models are difficult to use in autonomous driving, as they require too much training time to learn useful and general control policies. This problem can be largely alleviated if the reinforcement learning agent has a meaningful representation of its surrounding environment. Representation learning is a set of techniques to achieve this goal. In this work, we develop a novel graph neural network and train it to reconstruct important information of the surrounding environment from the perspective of the ego agent. We further extract an encoded hidden state from the model which can be used as a meaningful representation of the surrounding environment for reinforcement learning agents. . Introduction . One of the most challenging tasks for Deep Reinforcement Learning (DRL)agents is learning accurate representations of their environment. This constitutes a limiting factor for DRL as it results in long training time for agents. Including state representation learning objectives into the reward function has been shown to improve representations and increase generalization 1. Meaningful state representations are a cornerstone of accurate predictions in deep learning. Powerful representations encode relevant information and omit unnecessary details. Therefore, they reduce the complexity of following learning problems 2 such as reinforcement learning. . Manual feature engineering is the most common approach to building representations. However, there are many techniques to let a neural network learn strong representations and eliminate the need for labor-intensive feature handcrafting. These techniques are grouped by the term representation learning. . We apply representation learning to the field of autonomous driving and more specifically traffic modeling. The behavior of agents is highly dependent on the surrounding traffic participants. The agents and their interaction in a driving scenario can be considered as a graph in which the nodes and edges represent the agents and their interaction respectively. . Graph Neural Networks (GNNs) are neural network architectures that are used for deep learning on graph-structured data 3. We use them for representation learning on traffic scenes. GNNs enable node-level, edge-level and graph-level predictions. . The remainder of this report is organized as follows: Section II introduces relevant concepts. In Section III we present our approach to building meaningful representations of traffic scenes and in Section IV we introduce our implementation. In Section V, we show our experiments. In Section VI we conclude our work and present ideas for further research. . Preliminaries . Graph Neural Networks . Traffic scenarios can be represented effectively using graphs. A graph $G$ is a data structure that consists of a set of nodes (or vertices) $V$ and a set of edges $E$, i.e., $G=(V,E)$. $e_{ij}=(v_i,v_j) in E $ denotes an edge pointing from $v_j$ to $v_i$, where $v_i in V$. $N(v) = {u in V mid (v,u) in E}$ denotes the neighborhood of a node $v$. The node features $ mathbf{h} in mathbf{R}^{n times d} $ are defined as h={h⃗i∣i=1,...,n} mathbf{h} = lbrace vec{h}_i mid i=1,...,n rbraceh={hi​∣i=1,...,n}, where h⃗i∈Rd vec{h}_i in mathbf{R}^dhi​∈Rd represents the feature vector of the node iii, n=∣V∣n = vert V vertn=∣V∣ denotes the number of nodes and ddd denotes the dimension of the node feature vector. The edge features e∈Rm×c mathbf{e} in mathbf{R}^{m times c}e∈Rm×c are defined as {e⃗ij∣i=1,...,n,j=1,...,N(i)} { vec{e}_{ij} mid i=1,...,n,j=1,...,N(i) }{eij​∣i=1,...,n,j=1,...,N(i)}, where e⃗ij∈Rc vec{e}_{ij} in mathbf{R}^{c}eij​∈Rc represent the feature vector of the edge (i,j)(i,j)(i,j), m=∣E∣m = vert E vertm=∣E∣ denotes the number of edges and ccc denotes the dimension of the edge feature vector. . GNN uses a form of neural message passing (MPNN) to learn graph-structured data. MPNN treats graph convolutions as a message passing process in which vector messages can be passed from one node to another along edges directly. MPNN runs $L$ step message passing iterations to let messages propagate further4. The message passing function at message passing step $l$ is defined as h⃗il=fn(h⃗il−1,mil) vec{h}^{l}_i = f_n( vec{h}^{l-1}_i,m^{l}_i)hil​=fn​(hil−1​,mil​), where mil=Φ({e⃗ij l}j∈N(i))m^{l}_i = Phi( { vec{e}^{ ,l}_{ij} }_{j in N(i)})mil​=Φ({eijl​}j∈N(i)​), e⃗ij l=fe(h⃗il−1,h⃗jl−1,e⃗ij l−1) vec{e}^{ ,l}_{ij} = f_e( vec{h}^{l-1}_i, vec{h}^{l-1}_j, vec{e}^{ , l-1}_{ij})eijl​=fe​(hil−1​,hjl−1​,eijl−1​). milm^{l}_imil​ represents the message of node $i$ at message passing step $l$, Φ PhiΦ denotes an aggregation operation, fnf_nfn​ and fef_efe​ are functions with learnable parameters. . Edge Convolution . The edge convolution (Edge Conv) 5 we use is an asymmetric edge function. It operates the edges connecting neighboring pairs of nodes. Specifically, it updates the target node features by Eq. [test][1]. The operation captures the hidden information from the target node feature h⃗i vec{h}_ihi​ and also the neighborhood information, captured by h⃗j−h⃗i vec{h}_j- vec{h}_ihj​−hi​. . h⃗i′=max⁡j∈NiMLPθ([h⃗i,h⃗j−h⃗i]) vec{h}_i&amp;#x27; = max_{j in mathcal N_i} text{MLP}_{ theta}([ vec{h}_i, vec{h}_j- vec{h}_i])h . i′​=j∈Ni​max​MLPθ​([h . i​,h . j​−h . i​]) . The concatenated vector [h⃗i,h⃗j−h⃗i][ vec{h}_i, vec{h}_j- vec{h}_i][hi​,hj​−hi​] is transformed by a Multilayer perceptron (MLP) and then aggregated by a max operation. . Edge-Featured Graph Attention Network . Edge-Featured Graph Attention Networks (EGAT) 6 are an extension of Graph Attention Neural Networks (GAT) 7. Compared to GATs, EGATs allow for implicitly assigning different importances to different neighbor nodes, considering not only node features but also edge features. They don’t depend on knowing the entire graph structure upfront. Additionally, EGAT is computationally efficient. It does not require costly matrix operations. We use the node attention block of EGAT layer in our experiment. A node attention block of EGAT layer takes both node features h mathbf{h}h and edge features e mathbf{e}e as input and produces a new set of node features h′ mathbf{h}&amp;#x27;h′ as output, where h′={h⃗i′∣i=1,...,n} mathbf{h}&amp;#x27;= { vec{h}_i&amp;#x27; mid i=1,...,n }h′={hi′​∣i=1,...,n}. . Node and edge feature transformation . First, the node features h mathbf{h}h and edge features e mathbf{e}e are transformed by a linear layer (Eq. 2, 3), . h∗=Wh⋅hEq. X mathbf{h}^* = mathbf{W}_{h} cdot mathbf{h} text{Eq. X}h∗=Wh​⋅hEq. X . e∗=We⋅e mathbf{e}^* = mathbf{W}_{e} cdot mathbf{e}e∗=We​⋅e . where h∗ mathbf{h}^*h∗ and e∗ mathbf{e}^*e∗ are the projected node features and edge features respectively. . Edge enhanced attention . Given a target node $i$, the attention coefficient αi,j alpha_{i,j}αi,j​ is calculated by Eq. [c3]{reference-type=”ref” reference=”c3”}, αi,j alpha_{i,j}αi,j​ indicates the importance of the node $j$ to node $i$ jointly considering node and edge features. . αi,j=exp⁡(LeakyReLu(aT[h⃗i∗∥h⃗j∗∥e⃗ij∗])∑k∈N(i)exp⁡(LeakyReLu(aT[h⃗i∗∥h⃗j∗∥e⃗ij∗]) alpha_{i,j}= frac{ exp( text{LeakyReLu}( mathbf{a}^ mathrm{T}[ vec{h}_i^{*} Vert vec{h}_j^{*} Vert vec{e}_{ij}^*])}{ sum_{k in N(i)} exp( text{LeakyReLu}( mathbf{a}^ mathrm{T}[ vec{h}^*_{i} Vert vec{h}^*_{j} Vert vec{e}^*_{ij}])}αi,j​=∑k∈N(i)​exp(LeakyReLu(aT[h . i∗​∥h . j∗​∥e . ij∗​])exp(LeakyReLu(aT[h . i∗​∥h . j∗​∥e . ij∗​])​ . Here, a mathbf{a}a is a linear layer and N(i)N(i)N(i) is the neighbor nodes of node i in the graph. The node feature h⃗i′ vec{h}^{&amp;#x27;}_{i}hi′​ is then updated by calculating a weighted sum of edge-integrated node features over its neighbor nodes, followed by a sigmoid function. . h⃗i′=σ(∑j∈N(i)αijWhT[e⃗ij∗∥hj∗⃗]) vec{h}&amp;#x27;_{i}= sigma left( sum_{j in N(i)} alpha_{ij} mathbf{W}_h^ mathrm{T}[ vec{e}^*_{ij} Vert vec{h^*_{j}}] right)h . i′​=σ⎝ . ⎛​j∈N(i)∑​αij​WhT​[e . ij∗​∥hj∗​ . ​]⎠ . ⎞​ . Similar to GAT, we apply multi-head attention and run several independent attention mechanisms to get a stable self-attention mechanism output. Additionally, it allows the model to jointly attend to the information from different representation sub-spaces at different positions. The output of each attention head is concatenated as the final updated node feature h⃗i+ vec{h}^{+}_{i}hi+​. . h⃗i+=∥k=1Kh⃗ik′ vec{h}^{+}_{i}= mathop{ Vert} limits_{k=1}^{K} vec{h}^{k&amp;#x27;}_{i}h . i+​=k=1∥K​h . ik′​ . Here, $K$ is the number of attention heads and ∥ mathop{ Vert}∥ indicates the concatenation operation. . Methodology . In this section, we introduce the approach of constructing the graph data from the highD dataset and describe the definition of maximum closeness which is the ground truth label for reconstruction. We elaborate our reconstruction neural network and the loss function we used in the training process. Successful reconstruction of the important information of the agent environment indicates that the model is able to extract the latent information from the surrounding environment. Thus, we can extract an encoder output from the model and use it as a meaningful representation of the surrounding environment for reinforcement learning. Fig. 1{reference-type=”ref” reference=”encoder”} illustrates the basic concept of extracting the encoder output from the model pipeline. . . Graph extraction . The basis of our GNN training data is the highD dataset 8, a dataset of vehicle trajectories on highways. We extract fully connected graphs from highD. In our training dataset, graph nodes represent traffic participants and edges represent the relationship between different traffic participants. For node $i$, the position (xi,yi)(x_i, y_i)(xi​,yi​) acts as the node feature h⃗i vec{h}_ihi​. The edge feature e⃗ij vec{e}_{ij}eij​ for the edge between node $i$ and node $j$ consists of the Euclidean distance dijd_{ij}dij​ (Eq. [euclidean_distance]{reference-type=”ref” reference=”euclidean_distance”}), sine of the relative angle sin⁡(αij) sin( alpha_{ij})sin(αij​) (Eq. [angle_sin]{reference-type=”ref” reference=”angle_sin”}) and cosine of the relative angle cos⁡(αij) cos( alpha_{ij})cos(αij​) (Eq. [angle_cos]{reference-type=”ref” reference=”angle_cos”}). . dij=(xi−xj)2+(yi−yj)2d_{ij} = sqrt{(x_i-x_j)^2+(y_i-y_j)^2}dij​=(xi​−xj​)2+(yi​−yj​)2​ sin⁡αij=yi−yjdij sin alpha_{ij} = frac{y_i-y_j}{d_{ij}}sinαij​=dij​yi​−yj​​ cos⁡αij=xi−xjdij cos alpha_{ij} = frac{x_i-x_j}{d_{ij}}cosαij​=dij​xi​−xj​​ . Thus, our constructed graph consists of the node feature vector h mathbf{h}h (Eq. $12$) and the edge feature vector e mathbf{e}e (Eq. $13$). . h⃗i=[xi,yi] vec{h}_i = [x_i, y_i]h . i​=[xi​,yi​] . e⃗ij=[dij,sin⁡(αij),cos⁡(αij)] vec{e}_{ij} = [d_{ij}, sin( alpha_{ij}), cos( alpha_{ij})]e . ij​=[dij​,sin(αij​),cos(αij​)] . h={h⃗i∣i=1,...,N} mathbf{h} = { vec{h}_i mid i=1,...,N }h={h . i​∣i=1,…,N} . e={e⃗ij∣i=1,...,N,j=1,...,Ni} mathbf{e} = { vec{e}_{ij} mid i=1,...,N,j=1,...,N_i }e={e . ij​∣i=1,…,N,j=1,…,Ni​} . Maximum closeness . We divide the area surrounding the ego agent into eight 45∘45^{ circ}45∘ segments R={Ri∣i=1,...,8} mathbf{R} = {R_{i} mid i=1,...,8 }R={Ri​∣i=1,...,8} as illustrated in Fig. 2{reference-type=”ref” reference=”Regions”}, referred to as angular regions in the following. . . We define closeness ci,j∈[0,1]c_{i,j} in [0,1]ci,j​∈[0,1] (Eq. [closeness]{reference-type=”ref” reference=”closeness”}) as our proximity measure between the node $i$ (ego agent) and node $j$ (other traffic participants). Unlike the euclidean distance, closeness provides a smooth label and prevents discontinuities resulting from empty regions. ci,jc_{i,j}ci,j​ is $0$ for all values greater than or equal to DmaxD_{max}Dmax​ and $1$ if the euclidean distance di,jd_{i,j}di,j​ is $0$. The maximum closeness of node $i$ in angular region $m$ is defined as ci,m+{c}^{+}_{i,m}ci,m+​ (Eq. [closeness_angular_region]{reference-type=”ref” reference=”closeness_angular_region”}). Our ground truth label, c+ mathbf{c}^{+}c+ (Eq. [closeness_ground_truth]{reference-type=”ref” reference=”closeness_ground_truth”}), is the vector of the maximum closenesses. . ci,j=1−min⁡(di,j,Dmax)Dmaxc_{i,j} = 1 - frac{ min(d_{i,j},D_{max})}{D_{max}}ci,j​=1−Dmax​min(di,j​,Dmax​)​ . ci,m+=max⁡j∈Rm{cij}c^{+}_{i,m} = max_{j in mathcal{R}_m} {c_{ij} }ci,m+​=j∈Rm​max​{cij​} . c+={ci,m+∣i=1,...,N,m=1,...,8} mathbf{c}^{+} = {c^{+}_{i,m} mid i=1,...,N,m=1,...,8 }c+={ci,m+​∣i=1,…,N,m=1,…,8} . Model . We describe our general pipeline in Fig.3{reference-type=”ref” reference=”model”} We construct a fully connected graph to represent the agent environment. Our model takes the fully connected graph, i.e., all node feature vectors h mathbf{h}h and edge feature vectors e mathbf{e}e as inputs and outputs the maximum closeness of each target node in the eight regions, i.e., c+ mathbf{c}^{+}c+. Additionally, we extract the output of the encoder which can be used as the input for reinforcement learning. . . Classifier and regressor . We divide this maximum closeness reconstruction task into a binary classification task and a regression task. We train a classifier to predict the existence of agents in each region. Simultaneously, we train a regressor to reconstruct the maximum closeness in the regions in which agents exist. The final output is then calculated as the element-wise multiplication of classification output and regression output. . Encoding graph information . Each node attribute has two features: xix_ixi​ and yiy_iyi​ position. Each edge attribute has three features: relative distance dijd_{ij}dij​, sine of relative angle sin⁡(αij) sin( alpha_{ij})sin(αij​) and cosine of relative angle cos⁡(αij) cos( alpha_{ij})cos(αij​). Thus the node input and edge input has shape (n,2)(n,2)(n,2) and (m,3)(m,3)(m,3) respectively, where nnn is the number of nodes and $m$ is the number of edges. . We use a linear layer followed by two Edge Conv layers to process node input and obtain a (n,128)(n,128)(n,128) tensor: EnodeE_{node}Enode​. Similarly, we use a linear layer to process edge input and get a (m,64)(m,64)(m,64) tensor: EedgeE_{edge}Eedge​. . Extracting encoder output . We use two node attention blocks of EGAT to aggregate EnodeE_{node}Enode​ and EedgeE_{edge}Eedge​ for the classifier and regressor separately, so to obtain a (n,128)(n,128)(n,128) tensor: EclsE_{cls}Ecls​ and a (n,128)(n,128)(n,128) tensor: EregE_{reg}Ereg​. The encoder output EencoderE_{encoder}Eencoder​ is defined as the concatenation of EclsE_{cls}Ecls​ and EregE_{reg}Ereg​. . Decoding graph information . We use two linear layers as the classification decoder and regression decoder. The activation function of the last layer in both decoders is a sigmoid function, so we obtain an output value between $0$ and $1$. Then, the classification output is set to $1$ if its value is greater than $0.5$ and $0$ otherwise. The model output is calculated as regression output element multiplied by classification output. . Loss function . This model is trained with a classification loss Lcls mathcal{L}_{cls}Lcls​ and a regression loss Lreg mathcal{L}_{reg}Lreg​. The total loss is calculated in Eq. [loss]{reference-type=”ref” reference=”loss”}, where the design parameter β betaβ allows to balance the weight of the regression loss in relation to the classification loss. . L=βLreg+Lcls mathcal{L}= beta mathcal{L}_{reg} + mathcal{L}_{cls}L=βLreg​+Lcls​ . Implementation . Our implementation uses the CommonRoad-Geometric package developed at the chair of Robotics, Artificial Intelligence and Embedded Systems at the Technical University of Munich (TUM). CommonRoad-Geometric is a geometric deep learning library for autonomous driving that we use to extract graph data from traffic scenarios. It is built on top of PyTorch Geometric 9 and the CommonRoad framework 10. CommonRoad is a collection of benchmarks for autonomous driving that enable reproducibility and PyTorch Geometric is a popular PyTorch 11 based library for deep learning on graphs. As explained in Subsection 3.1{reference-type=”ref” reference=”subsection:graph_extraction”}, we use highD as the basis of our training data. . . . . Our package traffic_scene_representation is split into three subpackages: extraction, model and visualization (see Fig. [implementation]{reference-type=”ref” reference=”implementation”}). The extraction package contains two classes: extractor which extends the CommonRoad Geometric class StandardTrafficExtractor and collector which extends the CommonRoad Geometric class BaseDatasetCollector. extractor is responsible for extracting one PyTorch Geometric graph data object from a single scenario and timestep as described in Subsection 3.1{reference-type=”ref” reference=”subsection:graph_extraction”}. The class collector is responsible for collecting the entire training and test dataset from the highD dataset and saving the files to the disk. Furthermore, we built a GNN model as described in 3.3{reference-type=”ref” reference=”subsection:model”} using PyTorch Geometric. For building the GNN model, we implement three neural network layers, namely the classes EGATConvs, EdgeConv and MLP. The entire model is implemented in the class TrafficRepresentationNet. Additionally, we provide the tools to perform PCA decomposition on all encoder outputs. We also provide a simple set of visualization scripts, combined in the subpackage visualization, that can render the different traffic participants, the lanes and the prediction results. It can also visualize the PCA decomposition heatmap. This is useful for debugging and showcasing purposes. It is written using the Python libraries Pillow, a popular Python library for image manipulation and matplotlib, a commonly use plotting library. . Experiments . Training details . We use our constructed PyTorch Geometric graph dataset in our experiment. It contains $80000$ training samples, $10000$ validation samples and $10000$ test samples. We train our models for $50$ epochs with batch size $64$, using the Adam optimizer initialized with a learning rate of $0.001$ and exponential decay rate of $0.9$. All Edge Conv layers and EGAT layers are followed by BatchNormalization and ReLU activation. All linear layers except the last linear layer in both decoders are followed with ReLU activation. The classification loss Lcls mathcal{L}_{cls}Lcls​ is binary cross-entropy loss and the regression loss Lreg mathcal{L}_{reg}Lreg​ is Huber loss. We set the hyperparameter β betaβ in loss function to $100$. . . Reconstruction performance by using different encoder dimensions . We set up experiments for testing the performance of model using different encoder dimensions. We set the feature dimension of EclsE_{cls}Ecls​ and EregE_{reg}Ereg​ to 8,16,32,64,1288, 16, 32, 64, 1288,16,32,64,128 in five experiments respectively, simultaneously, modify the input dimension of the following linear layer to the same dimension and keep other hyperparameters unchanged. The converged loss of each encoder dimension is illustrated in Fig. 4{reference-type=”ref” reference=”converged loss”}. We can see that models with higher number of dimensions in the encoder output archive a lower converged loss than models with few dimensions. . Additionally, we observe that even using an encoder dimension of $8$, the model still performs very well. It indicates that we can use a low-dimensional encoder output to represent the agent environment and that we can use it as the input for reinforcement learning agents. . Reconstruction result and interpretation of the learned representations . Fig. 5{reference-type=”ref” reference=”scenario-explanation”} visualizes the result of the maximum closeness reconstruction of an arbitrary traffic scenario. The red rectangle represents the ego agent and the blue rectangle depicts the other traffic participants. The maximum closeness predictions for each angular region are illustrated in green. Positions and sizes of all traffic participants originate from our dataset, while the maximum closeness is reconstructed by the trained model. The direction of travel is from left to right for all traffic scenarios. We can see that our model can predict the maximum closeness well. . . To understand our learned representations better, we perform Principal Component Analysis (PCA) on the encoder output of the entire test dataset. Using PCA we can identify clusters in the learned representations. PCA derives a low-dimensional feature set from a higher-dimensional feature set while striving to preserve as much information (i.e. variance) as possible. . We show the results of $2$D PCA decomposition with point density heatmap and $3$D PCA decomposition in Fig.[pca-2D]{reference-type=”ref” reference=”pca-2D”} and Fig.[pca-3D]{reference-type=”ref” reference=”pca-3D”} respectively. The variance ratio of the three principal components are 15.17%15.17 %15.17%, 13.69%13.69 %13.69% and 8.37%8.37 %8.37% respectively. Both $2$D PCA and $3$D PCA show clear clustering. . . . We interpret the PCA results by visualizing traffic scenes at important points of the $2$D PCA heatmap. The most common cluster in the $2$D PCA heatmap is the cluster around the black dot in Fig. 8, corresponding to the scenario in Fig. [scenario:2]{reference-type=”ref” reference=”scenario:2”} in which there are close traffic participants around the ego agent. The cluster around the green dot in Fig. 8 is the second most common cluster in the $2$D PCA heatmap. This cluster corresponds to scenarios like Fig. [scenario:1]{reference-type=”ref” reference=”scenario:1”} in which there are fewer close traffic participants around ego agent compared to the main cluster. In addition, we observed that clusters in the heatmap with lower point density correspond to scenarios in which, either there are only a few close traffic participants in front of the ego agent, or behind of the ego agent. One example is illustrated in Fig. [scenario:4]{reference-type=”ref” reference=”scenario:4”}, which corresponds to the red dot in Fig. 8. . We can conclude, that clusters in the learned representations correspond to actual clusters in the surrounding environment from the perspective of the ego agent. It indicates the learned representations contain the latent information from the surrounding environment and can be used as meaningful representations for subsequent reinforcement learning problems. . []{#pca:points label=”pca:points”} . []{#pca-interpretation label=”pca-interpretation”} . . . . Conclusion . In this work, we have shown that modeling traffic scenes as graphs and using GNNs to learn representations of these traffic scenes leads to powerful state representations. In future work, we will use our learned representations as the input of reinforcement learning agents for motion planning. The representations can be further improved by extending the reconstruction task to predict the relative angle between the ego agent and the nearest agent in each region. In addition, a sequential model could be developed to predict the closeness between the ego agent and other traffic participants in future timestamps for a better representation. . Appendix . . De Bruin, Tim, et al. “Integrating state representation learning into deep reinforcement learning.” IEEE Robotics and Automation Letters 3.3 (2018): 1394-1401. &#8617; . | Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016. &#8617; . | Gori, Marco, Gabriele Monfardini, and Franco Scarselli. “A new model for learning in graph domains.” Proceedings. 2005 IEEE international joint conference on neural networks. Vol. 2. No. 2005. 2005. &#8617; . | Gilmer, Justin, et al. “Neural message passing for quantum chemistry.” International conference on machine learning. PMLR, 2017. &#8617; . | Wang, Yue, et al. “Dynamic graph cnn for learning on point clouds.” Acm Transactions On Graphics (tog) 38.5 (2019): 1-12. &#8617; . | Chen, Jun, and Haopeng Chen. “Edge-featured graph attention network.” arXiv preprint arXiv:2101.07671 (2021). &#8617; . | Veličković, Petar, et al. “Graph attention networks.” arXiv preprint arXiv:1710.10903 (2017). &#8617; . | Krajewski, Robert, et al. “The highd dataset: A drone dataset of naturalistic vehicle trajectories on german highways for validation of highly automated driving systems.” 2018 21st International Conference on Intelligent Transportation Systems (ITSC). IEEE, 2018. &#8617; . | Fey, Matthias, and Jan Eric Lenssen. “Fast graph representation learning with PyTorch Geometric.” arXiv preprint arXiv:1903.02428 (2019). &#8617; . | Althoff, Matthias, Markus Koschi, and Stefanie Manzinger. “CommonRoad: Composable benchmarks for motion planning on roads.” 2017 IEEE Intelligent Vehicles Symposium (IV). IEEE, 2017. &#8617; . | Paszke, Adam, et al. “Pytorch: An imperative style, high-performance deep learning library.” Advances in neural information processing systems 32 (2019). &#8617; . |",
            "url": "https://blog.cwallenwein.com/representation%20learning/2022/02/22/Graph-Representations-for-Predictive-Modeling-in-Traffic-Scenes.html",
            "relUrl": "/representation%20learning/2022/02/22/Graph-Representations-for-Predictive-Modeling-in-Traffic-Scenes.html",
            "date": " • Feb 22, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Through the Looking Glass - Neural 3D Reconstruction of Transparent Shapes",
            "content": ". Introduction . Obtaining the 3D shape of an object from a set of images is a well-studied problem. The corresponding research field is called Multi-view 3D-reconstruction. Many proposed techniques achieve impressive results but fail to reconstruct transparent objects. Image-based transparent shape reconstruction is an ill-posed problem. Reflection and refraction lead to complex light paths and small changes in shape might lead to completely different appearance. Different solutions to this problem have been proposed, but the acquisition setup is often tedious and requires a complicated setup. In 2020 a group of researchers from the University of California in San Diego state, they have found a technique that enables the reconstruction of transparent objects using only a few unconstrained images taken with a smartphone. This blog post will provide an in-depth look into the paper “Through the Looking Glass: Neural 3D Reconstruction of Transparent Shapes” by Zhengqin Li, Yu-Ying Yeh and Manmohan Chandraker1. . Previous work . Transparent object reconstruction has been studied for more than 30 years 2. A full history of related research is out of scope. Approaches were typically based on physics, but recently also deep learning researchers have attempted to find a solution. “Through the looking glass” is able to solve this challenging problem by combining the best of both worlds. The following is a brief overview of important work of the last years. A common characteristic of both approaches is the use of a synthetic dataset. . Physics-based approaches . The foundation of most research is the vast array of knowledge in the field of optics. The authors use this knowledge to formulate their mathematical models and improve them utilizing one manifestation of optimization algorithms. Recent research 3,4 uses synthetic datasets to overcome the hurdle of acquiring enough training data. This comes at the cost of a mismatch between the performance on real-world data compared to synthetic datasets. When testing on real-world data, these approaches typically require a complicated setup including multiple cameras taking images from various fixed viewpoints. A paper from 2018 by Wu et al. 4 captures 22 images of the transparent object in front of predefined background patterns. The camera viewpoints are fixed, but the transparent object rotates on a turntable. . The underlying optics concepts used in physics-based papers are fundamental to understand “Through the looking glass”. The section Overview of optics fundamentals will introduce these topics. . Deep Learning-based approaches . The setup of deep learning-based approaches is usually simpler. Using RGB 5/RGB-D6 images of the transparent object, models learn to predict e.g. the segmentation mask, the depth map and surface normals. These models are typically based on Encoder-Decoder CNNs. Deep learning methods inherently need far more data and therefore also leverage synthetic datasets. . Important concepts . Prior to introducing the proposed methods, some basic concepts from the fields of optics and 3D graphics shall be clarified. . Normals and color mapping . A normal is a vector that is perpendicular to some object. A surface normal is a vector that is perpendicular to a surface and it can be represented like any other vector $[X,Y,Z]$. . A normal map encodes the surface normal for each point in an image. The color at a certain point indicates the direction of the surface at this particular point. The color at each point is described by the three color channels $[R,G,B]$. A normal map simply maps the $[X,Y,Z]$-direction of the surface normals to the color channels. . Optics fundamentals . . Light propagates on straight paths through vacuum, air and homogeneous transparent materials. At the interface of two optically different materials, the propagation changes: In most configurations, a single path is split into two paths. For large angles, all light reflects back into the object and no light refracts. This is called total internal reflection. Snell’s law of refraction and the Fresnel equations allow calculating precise angles of reflection and refraction and the fraction of reflected and refracted light. In an image acquisition situation, beam splitting creates superimposed images. The higher the index of refraction (IOR, denoted $ text{n}_1$ and $ text{n}_2$ in Fig. 2), the slower the light travels in the optically dense matter, the stronger the surface reflection and the higher the angles of refraction and the shift of the refracted image. More about these concepts can be found at 7, 8 and 9. . Proposed method . Problem setup . . The inputs to the model are . 5-20 unconstrained images of the transparent object, | the corresponding silhouette segmentation masks for all images, | the environment map, | the index of refraction of the transparent material. | . . The proposed method limits the light path simulation to a maximum of two bounces. The camera looks onto the transparent object. . Overview . The authors propose the following contributions: . A physics-based network that estimates the front and back normals for a single viewpoint. It leverages a fully differentiable, also physics-based, rendering layer. | Physics-based point cloud reconstruction using the predicted normals. | A publicly available synthetic dataset 10 containing 3600 transparent objects. Each object is captured in 35 images from random viewpoints, resulting in a total of 120,000 high-quality images. | . The model starts off by initializing a point cloud of the transparent shape. This point cloud is inaccurate but serves as a good starting point for further optimization. In the next step, the physics-based neural network estimates the normal maps $N^1$ and $N^2$ for each viewpoint. The predicted, viewpoint-specific features, will then be mapped onto the point cloud. Finally, the authors use point cloud reconstruction to recover the full geometry of the transparent shape. The model is trained using the synthetic dataset and can be tested on real-world data. The code is publicly available on Github 11. . Space carving . Given a set of segmentation masks and their corresponding viewpoint, the space carving algorithm, first introduced by Kutulakos et al. more than 20 years ago, is able to reconstruct a good estimate of the ground truth shape, called visual hull (paper12, intuitive video13). Front and back normal maps can be calculated from the visual hull. They will later be referred to by the notation $ tilde{N^1}$, $ tilde{N^2}$, or by the term visual hull initialized normals. These normals already provide a good estimate for the ground truth normals. . Normal prediction . Differentiable rendering layer . The model utilizes differentiable rendering to produce high-quality images of transparent objects from a given viewpoint. The renderer is physics-based and uses the Fresnel equations and Snell’s law to calculate complex light paths. Differentiable rendering is an exciting, new field that emerged in the last years as it allows for backpropagation through the rendering layer. This video 14 provides a good introductory overview of the topic. To render the image from one viewpoint, the differentiable rendering layer requires the environment map $E$ and the estimated normal maps $N^1$, $N^2$ for this particular viewpoint. It outputs the rendered image, a binary mask indicating points where total internal reflection occurred and the pointwise rendering error with masked out environment. The rendering error map is calculated by comparing the rendered image to the ground truth image, cf. Fig. 5. . . Cost volume . The search space to find the correct normal maps $N^1$, $N^2$ is enormous. Each point in the front and back normal map could have a completely different surface normal. As stated before, the visual hull initialized normal maps are a good estimate for the ground truth normal maps. Therefore, the search space will be restricted to normal maps close to the visual hull initialized normals. To further reduce the search space, $K$ normal maps for the front and back surface are randomly sampled around the visual hull initialized normal maps. $K$ normal maps for both $N^1$ and $N^2$ lead to $K times K$ combinations. According to the authors, $K=4$ gives good results. Higher values for $K$ only increase the computational complexity without improving the quality of the normal estimations. The entire cost volume consists of the front and back normals, the rendering error and the total internal reflection mask, cf. Fig. 6. . . Normal prediction network . To estimate the surface normals an encoder-decoder CNN is used. The cost volume is still too large to be fed into the network. Therefore, the authors first use learnable pooling to perform feature extraction on the cost volume. They concatenate the condensed cost volume together with . the image of the transparent object | the image with masked out environment | the visual hull initialized normals | the total internal reflection mask | the rendering error | . and feed everything to the encoder-decoder CNN. $L_N= vert N^1 - hat{N^1} vert ^2 + vert N^2 - hat{N^2} vert^2$ is used as the loss function. It is simply the $L^2$ distance between the estimated normals ($N^1, N^2$) and the ground truth normals ($ hat{N^1}, hat{N^2}$). . Point cloud reconstruction . The normal prediction network gives important information about the transparent object from different viewing angles. But somehow the features in the different views have to be matched with the points in the point cloud. Subsequently, a modified PointNet++ 15 will predict the final point locations and final normal vectors for each point in the point cloud. Finally, the 3D point cloud will be transformed into a mesh by applying Poisson surface reconstruction 16. . Feature mapping . The goal of feature mapping is to assign features to each point in the initial point cloud. In particular, these features are the normal at that point, the rendering error and the total internal reflection mask. The features are known for each viewpoint but not for the points in the point cloud. A point of the point cloud can be mapped from 3D-space to the 2D point of each viewpoint to retrieve the necessary information. In some cases, a point might not be visible from a particular angle, if so, it will not be taken into account during feature mapping. For each point, there are usually 10 different views and sets of features. It now has to be decided, which view(s) to take into account when creating the feature vectors. The authors try three different feature mapping approaches, see section 3.2 Feature Mapping 1 for more details. Selecting the view with the lowest rendering error leads to the best results. . Modified PointNet++ . Given the mapped features and the initial point cloud, PointNet++ predicts the final point cloud and the corresponding normals. The authors were able to improve the predictions by modifying the PointNet++ architecture. In particular, they replaced max-pooling with average pooling, passed the front and back normals to all skip connections and applied feature augmentation. The best results are obtained with a chamfer-distance-based loss. The chamfer distance can be used to measure the distance between 2 point clouds. The authors try two other loss functions. For details see Table 2 of the paper 1. . Poisson surface reconstruction . Poisson surface reconstruction was first introduced in 2006 16 and is still used in recent papers. It takes a point cloud and surface normal estimations for each point of the point cloud and reconstructs the 3D mesh of the object. . Evaluation . Qualitative results . At first sight, the quality of the reconstructions looks really good. The scenes look reasonable and no big differences between ground truth and reconstructions are visible. . Testing on real-world data . To test the model on real-world data the following is needed: . images of the transparent object, | silhouette segmentation masks of all images, | environment map. | . The authors claim that real-world testing can be done by “light-weight mobile phone-based acquisition” 17. The images of the object can indeed be captured with commodity hardware like a smartphone camera. COLMAP18 is used to determine the viewpoint of the images. However, the segmentation masks were created by the authors manually, and to capture the environment map, a mirror sphere has to be placed at the position of the transparent object. . Comparison to latest SOTA . Quick reminder: this is deep learning research. It comes as no big surprise, that there is a newer paper 19, with a different approach, that works better. This newer paper is by Lyu et al. and it’s the successor of 4. Figure 5 shows the qualitative results of Li et al.1 compared to Lyu et al. The left side presents the results of Lyu et al. (1st column) compared with their ground truth (2nd column). On the right side, the results of Li et al. are displayed (ground truth: 3rd column, reconstruction: 4th column). . . It is clearly visible that the results of Li et al.1 are oversmoothed. There is no space between the hand and the head of the monkey. Additionally, neither the eyes of the dog nor the monkey are visible in the reconstructions. Lyu et al.19 on the other side successfully reconstructs the eyes of both animals and clearly separates the hand from the head of the monkey. One possible reason for this oversmoothing is the average pooling in the modified PointNet++. It has to be taken into account, however, that the underlying ground truth in both papers is slightly different and that Li et al.1 optimized for easy acquisition of the shape. Lyu et al.19 improved their acquisition ease compare to 4 but is still more restricted than Li et al.1. A quantitative comparison between both papers can be found in Table 1. It displays the reconstruction error in the form of the average per-vertex distance to the corresponding ground truth. Lyu et al. was able to cut the per-vertex distance approximately in half for all tested shapes. . Table 1: Reconstruction error of Li et al.1 and Lyu et al.19 (based on Table 1 of 19) .   initial Li et al. 20201 Lyu et al. 202019 . Mouse | 0.007164 | 0.005840 | 0.003075 | . Dog | 0.004481 | 0.002778 | 0.002065 | . Monkey | 0.005048 | 0.004632 | 0.002244 | . Pig | 0.004980 | 0.004741 | 0.002696 | . Conclusion . This paper proposed a novel approach that combined different paths and provides good results, but the reconstruction is not as effortless as claimed. . For the sake of simplicity, some relevant details were left out. In case of questions, read the paper 1 and the code 11 or send me an email. Thank you to Yu-Ying Yeh for clarifying questions about the paper, Eckhard Siegmann for suggestions and proofreading and Pengyuan for tips and advice. . References . Li, Zhengqin, Yu-Ying Yeh, and Manmohan Chandraker. “Through the looking glass: neural 3D reconstruction of transparent shapes.” Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020. &#8617; &#8617;2 &#8617;3 &#8617;4 &#8617;5 &#8617;6 &#8617;7 &#8617;8 &#8617;9 &#8617;10 . | Murase, Hiroshi. “Surface shape reconstruction of an undulating transparent object.” Proceedings Third International Conference on Computer Vision, 1990 &#8617; . | Qian, Yiming, Minglun Gong, and Yee-Hong Yang. “Stereo-based 3D reconstruction of dynamic fluid surfaces by global optimization.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2017. &#8617; . | Wu, Bojian, et al. “Full 3D reconstruction of transparent objects.” arXiv preprint arXiv:1805.03482 (2018). &#8617; &#8617;2 &#8617;3 &#8617;4 . | Stets, Jonathan, et al. “Single-shot analysis of refractive shape using convolutional neural networks.” 2019 IEEE Winter Conference on Applications of Computer Vision (WACV). IEEE, 2019. &#8617; . | Sajjan, Shreeyak, et al. “Clear grasp: 3d shape estimation of transparent objects for manipulation.” 2020 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2020. &#8617; . | The University of British Columbia, Department of Mathematics “The Law of Refraction”, accessed 16. July 2021, https://secure.math.ubc.ca/~cass/courses/m309-01a/chu/Fundamentals/snell.htm &#8617; . | The University of British Columbia, Department of Mathematics “The Law of Reflection”, accessed 16. July 2021, https://secure.math.ubc.ca/~cass/courses/m309-01a/chu/Fundamentals/reflection.htm &#8617; . | Wikipedia “Fresnel equations”, accessed 16. July 2021, https://en.wikipedia.org/wiki/Fresnel_equations &#8617; . | Transparent Shape dataset repository, 2020, uploaded by Zhengqin Li, https://github.com/lzqsd/TransparentShapeDataset &#8617; . | Transparent Shape paper repository, 2020, uploaded by Zhengqin Li, https://github.com/lzqsd/TransparentShapeReconstruction &#8617; &#8617;2 . | Kutulakos, Kiriakos N., and Steven M. Seitz. “A theory of shape by space carving.” International journal of computer vision 38.3 (2000): 199-218. &#8617; . | Computerphile “Space Carving - Computerphile”, 2016, https://www.youtube.com/watch?v=cGs90KF4oTc&amp;t=73s &#8617; . | UofT CSC 2547 3D &amp; Geometric Deep Learning “CSC2547 Differentiable Rendering A Survey”, 2021, https://www.youtube.com/watch?v=7LU0KcnSTc4 &#8617; . | Qi, Charles R., et al. “Pointnet++: Deep hierarchical feature learning on point sets in a metric space.” arXiv preprint arXiv:1706.02413 (2017). &#8617; . | Kazhdan, Michael, Matthew Bolitho, and Hugues Hoppe. “Poisson surface reconstruction.” Proceedings of the fourth Eurographics symposium on Geometry processing. Vol. 7. 2006. &#8617; &#8617;2 . | ComputerVisionFoundation Videos “Through the Looking Glass: Neural 3D Reconstruction of Transparent Shapes”, 2020, https://www.youtube.com/watch?v=zVu1v4rasAE&amp;t=53s &#8617; . | Schonberger, Johannes L., and Jan-Michael Frahm. “Structure-from-motion revisited.” Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. &#8617; . | Lyu, Jiahui, et al. “Differentiable refraction-tracing for mesh reconstruction of transparent objects.” ACM Transactions on Graphics (TOG) 39.6 (2020): 1-13. &#8617; &#8617;2 &#8617;3 &#8617;4 &#8617;5 &#8617;6 . |",
            "url": "https://blog.cwallenwein.com/computer%20vision/2021/07/16/Neural-3D-Reconstruction-of-Transparent-Shapes.html",
            "relUrl": "/computer%20vision/2021/07/16/Neural-3D-Reconstruction-of-Transparent-Shapes.html",
            "date": " • Jul 16, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://blog.cwallenwein.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://blog.cwallenwein.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}